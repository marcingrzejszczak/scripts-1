== Introduction

This section describes the rationale
behind the opinionated pipeline. We go through each deployment
step and describe it in detail.

IMPORTANT: You do not need to use all the pieces of Cloud Pipelines. You
can (and should) gradually migrate your applications to use those pieces of
Cloud Pipelines that you think best suit your needs.

=== Five-second Introduction

Cloud Pipelines provides scripts, configuration, and convention for automated
deployment pipeline creation with Cloud Foundry or Kubernetes.
We support various languages and frameworks. Since this project uses bash scripts,
you can use it with whatever automation server you have.

=== Five-minute Introduction

Cloud Pipelines comes with bash scripts (available under `src//main/bash`)
that represent the logic of all steps in our opinionated deployment pipeline.
Since we believe in convention over configuration, for the supported framework and
languages, we assume that the projects follow certain conventions of task naming,
profile setting, and so on. That way, if you create a new application,
your application can follow those conventions and the deployment pipeline works.
Since no one pipeline can serve the purposes of all
teams in a company, we believe that minor deployment pipeline tweaking should take place.
That is why we allow the usage of that `cloud-pipelines.yml` descriptor, which allows for
provide some customization.

==== How to Use It

This repository can be treated as a template for your pipeline. We provide some opinionated
implementation that you can alter to suit your needs. To use it, we recommend downloading
the Cloud Pipelines repository as a zip file, unzipping it in a directory,
initializing a Git project in that directory, and then modifying the project to suit your
needs. The following bash script shows how to do so:

====
[source,bash]
----
$ # pass the branch (e.g. master) or a particular tag (e.g. v1.0.0.RELEASE)
$ CLOUD_PIPELINES_RELEASE=...
$ curl -LOk https://github.com/CloudPipelines/scripts/archive/${CLOUD_PIPELINES_RELEASE}.zip
$ unzip ${CLOUD_PIPELINES_RELEASE}.zip
$ cd spring-cloud-pipelines-${CLOUD_PIPELINES_RELEASE}
$ git init
$ # modify the pipelines to suit your needs
$ git add .
$ git commit -m "Initial commit"
$ git remote add origin ${YOUR_REPOSITORY_URL}
$ git push origin master
----
====

To keep your repository aligned with the changes in the upstream repository, you can also
clone the repository. To not have many merge conflicts, we recommend using the `custom`
folder hooks to override functions.

Cloud Pipelines Scripts contains bash scripts that are required at runtime of
execution of a pipeline. If you want to read their documentation, it's
available under https://github.com/{org}/{repo}/blob/{branch}/src/main/bash/README.adoc[`src/main/bash/README.adoc`] file of Cloud Pipelines repository.

==== How It Works

As the following image shows, Cloud Pipelines contains logic to generate a
pipeline and the runtime to execute pipeline steps.

image::{intro-root-docs}/how.png[title="How Cloud Pipelines works"]

Once a pipeline is created (for example, by using the Jenkins Job DSL or from a Concourse
templated pipeline), when the jobs are ran, they clone or download Cloud Pipelines
code to run each step. Those steps run functions that are
defined in the `commons` module of Cloud Pipelines.

Cloud Pipelines performs steps to guess what kind of a project your
repository is (for example, JVM or PHP) and what framework it uses (Maven or Gradle), and it
can deploy your application to a cloud (Cloud Foundry or Kubernetes). You can read about how
it works by reading the <<how-do-the-scripts-work>> section.

All of that happens automatically if your application follows the conventions.
You can read about them in the <<project-opinions>> section.

[[deployment-languages-compatibility-matrix]]
==== Deployment & languages compatibility matrix

In the following table we present which language is supported by which deployment
mechanism.

.Deployment & languages compatibility matrix
|===
|Language | CF | K8S | Ansible

| JVM with Gradle
| ✅
| ✅
| ✅

| JVM with Maven
| ✅
| ✅
| ✅

| PHP with Composer
| ✅
| ✅
| ❌

| NodeJS with NPM
| ✅
| ✅
| ❌

| Dotnet core
| ✅
| ✅
| ❌

|===

TIP: For K8S, a deployment unit is a docker image so any language and framework
can be used.

==== Centralized Pipeline Creation

You can use Cloud Pipelines to generate pipelines
for all the projects in your system. You can scan all your
repositories (for example, you can call the Stash or Github API to retrieve the list of repositories)
and then:

* For Jenkins, call the seed job and pass the `REPOS`
parameter, which contains the list of repositories.
* For Concourse, call `fly` and set the
pipeline for every repository.

To achieve this you can use the https://github.com/CloudPipelines/project-crawler/[Project Crawler] library.

TIP: We recommend using Cloud Pipelines this way.

==== A Pipeline for Each Repository

You can use Cloud Pipelines in such a way that
each project contains its own pipeline definition in
its code. Cloud Pipelines clones the code with
the pipeline definitions (the bash scripts), so the
only piece of logic that needs to be in your application's
repository is the pipeline definition.

include::FLOW.adoc[]

=== Pipeline Descriptor

Each application can contain a file (called `cloud-pipelines.yml`) with the following structure:

====
[source,yaml]
----
language_type: jvm
pipeline:
	# used for multi module projects
	main_module: things/thing
	# used for multi projects
	project_names:
		- monoRepoA
		- monoRepoB
	# should deploy to stage automatically and run e2e tests
	auto_stage: true
	# should deploy to production automatically
	auto_prod: true
	# should the api compatibility check be there
	api_compatibility_step: true
	# should the test rollback step be there
	rollback_step: true
	# should the stage step be there
	stage_step: true
	# should the test step (including rollback) be there
	test_step: true
lowercaseEnvironmentName1:
	# used by spinnaker
	deployment_strategy: HIGHlANDER
	# list of services to be deployed
	services:
		- type: service1Type
		  name: service1Name
		  coordinates: value
		- type: service2Type
		  name: service2Name
		  key: value
lowercaseEnvironmentName2:
	# used by spinnaker
	deployment_strategy: HIGHlANDER
	# list of services to be deployed
	services:
		- type: service3Type
		  name: service3Name
		  coordinates: value
		- type: service4Type
		  name: service4Name
		  key: value
----
====

If you have a multi-module project, you should point to the folder that contains the
module that produces the fat jar. In the preceding example, that module
would be present under the `things/thing` folder. If you have a single module project,
you need not create this section.

For a given environment, we declare a list of infrastructure services that we
want to have deployed. Services have:

* `type` (examples: `eureka`, `mysql`, `rabbitmq`, and `stubrunner`): This value gets
then applied to the `deployService` Bash function
* *[KUBERNETES]*: For `mysql`, you can pass the database name in the `database` property.
* `name`: The name of the service to get deployed.
* `coordinates`: The coordinates that let you fetch the binary of the service.
It can be a Maven coordinate (`groupid:artifactid:version`),
a docker image (`organization/nameOfImage`), and so on.
* Arbitrary key value pairs, which let you customize the services as you wish.

==== Pipeline Descriptor for Cloud Foundry

When deploying to Cloud Foundry you can provide services
of the following types:

* `type: broker`
** `broker`: The name of the CF broker
** `plan`: The name of the plan
** `params`: Additional parameters are converted to JSON
** `useExisting`: Whether to use an existing one or
create a new one (defaults to `false`)
* `type: app`
** `coordinates`: The Maven coordinates of the stub runner jar
** `manifestPath`: The path to the manifest for the stub runner jar
* `type: cups`
** `params`: Additional parameters are converted to JSON
* `type: cupsSyslog`
** `url`: The URL to the syslog drain
* `type: cupsRoute`
** `url`: The URL to the route service
* `type: stubrunner`
** `coordinates`: The Maven coordinates of the stub runner jar
** `manifestPath`: The path to the manifest for the stub runner jar

The following example shows the contents of a YAML file that defines the preceding values:

====
[source,yaml]
----
# This file describes which services are required by this application
# in order for the smoke tests on the TEST environment and end to end tests
# on the STAGE environment to pass

# lowercase name of the environment
test:
  # list of required services
  services:
    - name: config-server
      type: broker
      broker: p-config-server
      plan: standard
      params:
        git:
          uri: https://github.com/ciberkleid/app-config
      useExisting: true
    - name: cloud-bus
      type: broker
      broker: cloudamqp
      plan: lemur
      useExisting: true
    - name: service-registry
      type: broker
      broker: p-service-registry
      plan: standard
      useExisting: true
    - name: circuit-breaker-dashboard
      type: broker
      broker: p-circuit-breaker-dashboard
      plan: standard
      useExisting: true
    - name: stubrunner
      type: stubrunner
      coordinates: io.pivotal:cloudfoundry-stub-runner-boot:0.0.1.M1
      manifestPath: sc-pipelines/manifest-stubrunner.yml

stage:
  services:
    - name: config-server
      type: broker
      broker: p-config-server
      plan: standard
      params:
        git:
          uri: https://github.com/ciberkleid/app-config
    - name: cloud-bus
      type: broker
      broker: cloudamqp
      plan: lemur
    - name: service-registry
      type: broker
      broker: p-service-registry
      plan: standard
    - name: circuit-breaker-dashboard
      type: broker
      broker: p-circuit-breaker-dashboard
      plan: standard
----
====

Another CF specific property is `artifact_type`. Its value can be either `binary` or `source`.
Certain languages (such as Java) require a binary to be uploaded, but others (such as PHP)
require you to push the sources. The default value is `binary`.

=== Project Setup

Cloud Pipelines supports three main types of project setup:

* `Single Project`
* `Multi Module`
* `Multi Project` (also known as mono repo)

A `Single Project` is a project that contains a single module that gets
built and packaged into a single, executable artifact.

A `Multi Module` project is a project that contains multiple modules.
After building all modules, one gets packaged into a single, executable artifact.
You have to point to that module in your pipeline descriptor.

A `Multi Project` is a project that contains multiple projects. Each of those
projects can in turn be a `Single Project` or a `Multi Module` project. Spring
Cloud Pipelines assume that, if a `PROJECT_NAME` environment
variable corresponds to a folder with the same name in the root of the
repository, this is the project it should build. For example, for
`PROJECT_NAME=something`, if there's a folder named `something`, then Cloud Pipelines
treats the `something` directory as the root of the `something` project.

[[how-do-the-scripts-work]]
== How the Scripts Work

This section describes how the scripts and jobs correspond to each other.
If you need to see detailed documentation of the bash scripts, go to the
code repository and read `src//main/bash/README.adoc`.

[[build-and-deployment]]
=== Build and Deployment

The following text image shows a high-level overview:

[plantuml, build-and-deployment, png]
----
script->pipeline: [source pipeline.sh]
note right of pipeline: loading functions, env vars
note left of pipeline: hopefully all functions get overridden \notherwise nothing will work
pipeline->projectType: Source the [projectType/pipeline-projectType.sh]
note left of projectType: What do we have here...? \nA [mvnw] file, \nit has to be a [jvm] project
projectType->language: Source [pipeline-jvm.sh]
language->framework: Maven or Gradle?
note right of framework: There's a [mvnw] file? \nSo the [PROJECT_TYPE] must be [maven]
framework->pipeline: It's a Maven project
pipeline->paas: The [PAAS_TYPE] is [cf] so I'll source [pipeline-cf.sh]
note right of paas: Loading all \ndeployment-related functions
note left of pipeline: Ok, we know that it's Maven \nand should be deployed to CF
pipeline->custom: Try to source [custom/build_and_upload.sh]
note right of custom: No such file so \nnothing custom to be done
note right of script: All build related functions \n overridden by language / framework scripts
note right of script: All deploy related functions \n overridden by paas scripts
script->script: run [build] function
----

Before we run the script, we need to answer a few questions related to your repository:

* What is your language (for example, `jvm`,`php`, or something else)?
* what framework do you use (for example, `maven` or `gradle`)?
* what PAAS do you use (for example, `cf` or `k8s`)?


The following sequence diagram describes how the sourcing of bash scripts takes place:

[plantuml, sourcing, png]
----
script->language: What is your language?
language->script: I'm written in X language
language->framework: What framework do you use?
framework->script: I use Y framework
script->paas: I know that you use Z PAAS?
paas->script: Here are all Z-related deployment functions
script->customization: Anything custom to override in bash?
customization->script: Not this time...
script->script: Ok, run the script
----

The process works as follows:

. A script (for example, `build_and_upload.sh`) is called.
. It sources the `pipeline.sh` script that contains all the essential function "`interfaces`" and
environment variables.
. `pipeline.sh` needs information about the project type. It
sources `projectType/pipeline-projectType.sh`.
. `projectType/pipeline-projectType.sh` contains logic to determine the language.
.. Verify whether a repository contains files that correspond to the given languages (for example, `mvnw` or `composer.json`).
.. Verify whether a concrete framework that we support (for example, `maven` or `gradle`) is present.
. Once we know what the project type is, we can deal with PAAS. Depending on the value of the `PAAS_TYPE` environment
variable, we can source proper PAAS functions (for example, `pipeline-cf.sh` for Cloud Foundry).
. Determine whether we can do some further customization.
.. Search for a file called `${sc-pipelines-root}/src//main/bash/custom/build_and_upload.sh`
to override any functions you want.
. Run the `build` function from `build_and_upload.sh`
