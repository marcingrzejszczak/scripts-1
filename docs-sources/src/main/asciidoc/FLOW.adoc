=== The Flow

The following images show the flow of the opinionated pipeline:

image::{intro-root-docs}/flow_concourse.png[title="Flow in Concourse"]

image::{intro-root-docs}/flow.png[title="Flow in Jenkins"]

We first describe the overall concept behind the flow and then
split it into pieces and describe each piece independently.

=== Vocabulary

This section defines some common vocabulary. We describe four typical
environments in terms of running the pipeline.

==== Environments

We typically encounter the following environments:

* *build* environment is a machine where the building of the application takes place.
It is a continuous integration or continuous delivery tool worker.
* *test* is an environment where you can deploy an application to test it. It does not
resemble production, because we cannot be sure of its state (which application is deployed
there and in which version). It can be used by multiple teams at the same time.
* *stage* is an environment that does resemble production. Most likely, applications
are deployed there in versions that correspond to those deployed to production.
Typically, staging databases hold (often obfuscated) production data. Most
often, this environment is a single environment shared between many teams. In other
words, in order to run some performance and user acceptance tests, you have to block
and wait until the environment is free.
* *prod* is the production environment where we want our tested applications to be
deployed for our customers.

==== Tests

We typically encounter the following kinds of tests:

* *Unit tests*: Tests that run on the application during the build phase.
No integrations with databases or HTTP server stubs or other resources take place.
Generally speaking, your application should have plenty of these tests to provide fast
feedback about whether your features work.

* *Integration tests*: Tests that run on the built application during the build phase.
Integrations with in-memory databases and HTTP server stubs take place. According to the
https://martinfowler.com/bliki/TestPyramid.html[test pyramid], in most cases, you should
not have many of these kind of tests.

* *Smoke tests*: Tests that run on a deployed application. The concept of these tests
is to check that the crucial parts of your application are working properly. If you have 100 features
in your application but you gain the most money from five features, you could write smoke tests
for those five features. We are talking about smoke tests of an application, not of
the whole system. In our understanding inside the opinionated pipeline, these tests are
executed against an application that is surrounded with stubs.

* *End-to-end tests*: Tests that run on a system composed of multiple applications.
These tests ensure that the tested feature works when the whole system is set up.
Due to the fact that it takes a lot of time, effort, and resources to maintain such an environment
and that these tests are often unreliable (due to many different moving pieces, such as network,
database, and others), you should have a handful of those tests. They should be only for critical parts of your business.
Since only production is the key verifier of whether your feature works, some companies
do not even want to have these tests and move directly to deployment to production. When your
system contains KPI monitoring and alerting, you can quickly react when your deployed application
does not behave properly.

* *Performance testing*: Tests run on an application or set of applications
to check if your system can handle a big load. In the case of our opinionated pipeline,
these tests can run either on test (against a stubbed environment) or on
staging (against the whole system).

==== Testing against Stubs

Before we go into the details of the flow, consider the example described by the following image:

image::{intro-root-docs}/monolith.png[title="Two monolithic applications deployed for end to end testing"]

When you have only a handful of applications, end-to-end testing is beneficial.
From the operations perspective, it is maintainable for a finite number of deployed instances.
From the developers perspective, it is nice to verify the whole flow in the system
for a feature.

In the case of microservices, the scale starts to be a problem, as the following image shows:

image::{intro-root-docs}/many_microservices.png[title="Many microservices deployed in different versions"]

The following questions arise:

* Should I queue deployments of microservices on one testing environment or should I have an environment per microservice?
** If I queue deployments, people have to wait for hours to have their tests run. That can be a problem
* To remove that issue, I can have an environment for each microservice.
** Who will pay the bills? (Imagine 100 microservices, each having each own environment).
** Who will support each of those environments?
** Should we spawn a new environment each time we execute a new pipeline and then wrap it up or should we have
them up and running for the whole day?
* In which versions should I deploy the dependent microservices - development or production versions?
** If I have development versions, I can test my application against a feature that is not yet on production.
That can lead to exceptions in production.
** If I test against production versions, I can never test against a feature under development
anytime before deployment to production.

One of the possibilities of tackling these problems is to not do end-to-end tests.

The following image shows one solution to the problem, in the form of stubbed dependencies:

image::{intro-root-docs}/stubbed_dependencies.png[title="Execute tests on a deployed microservice on stubbed dependencies"]

If we stub out all the dependencies of our application, most of the problems presented earlier
disappear. There is no need to start and setup the infrastructure required by the dependent
microservices. That way, the testing setup looks like the following image:

image::{intro-root-docs}/stubbed_dependencies.png[title="We're testing microservices in isolation"]

Such an approach to testing and deployment gives the following benefits
(thanks to the usage of http://cloud.spring.io/spring-cloud-contract/spring-cloud-contract.html[Spring Cloud Contract]):

* No need to deploy dependent services.
* The stubs used for the tests run on a deployed microservice are the same as those used during integration tests.
* Those stubs have been tested against the application that produces them (see http://cloud.spring.io/spring-cloud-contract/spring-cloud-contract.html[Spring Cloud Contract] for more information).
* We do not have many slow tests running on a deployed application, so the pipeline gets executed much faster.
* We do not have to queue deployments. We test in isolation so that pipelines do not interfere with each other.
* We do not have to spawn virtual machines each time for deployment purposes.

However, this approach brings the following challenges:

* No end-to-end tests before production. You do not have full certainty that a feature is working.
* The first time the applications interact in a real way is on production.

As with every solution, it has its benefits and drawbacks. The opinionated pipeline
lets you configure whether you want to follow this flow or not.

==== General View

The general view behind this deployment pipeline is to:

* Test the application in isolation.
* Test the backwards compatibility of the application, in order to roll it back if necessary.
* Allow testing of the packaged application in a deployed environment.
* Allow user acceptance tests and performance tests in a deployed environment.
* Allow deployment to production.

The pipeline could have been split to more steps, but it seems that all of the aforementioned
actions fit nicely in our opinionated proposal.
